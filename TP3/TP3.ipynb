{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Prático 03 - Implementação de Redes Neurais (NN e CNN)\n",
    "\n",
    "- Giovanna Louzi Bellonia - 2017086015\n",
    "- Thiago Martin Poppe - 2017014324"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import seaborn as ss\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação de uma classe para ler os dados do CIFAR-10\n",
    "\n",
    "- Baseado na leitura dos dados do MNIST presente nos slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10:\n",
    "    \"\"\" Class to read CIFAR-10 data \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_data():\n",
    "        \"\"\" Static method to read the data \"\"\"\n",
    "\n",
    "        (train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "\n",
    "        train_images = train_images.astype('float32')\n",
    "        test_images = test_images.astype('float32')\n",
    "        train_images /= 255\n",
    "        test_images /= 255\n",
    "        \n",
    "        # Criando os vetores de one-hot\n",
    "        train_labels = np_utils.to_categorical(train_labels, 10)\n",
    "        test_labels = np_utils.to_categorical(test_labels, 10)\n",
    "\n",
    "        return (train_images, train_labels), (test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação de uma classe para criar uma arquitetura LeNet-5\n",
    "- Nos baseamos na segunda versão da LeNet-5 disponibilizada nos slides.\n",
    "- A arquitetura consiste em 2 níves de convolução, com 20 e 50 filtros respectivamente (sempre seguidos de ReLU e MaxPooling2D 2x2).\n",
    "- Em seguida temos uma fully connected com 500 \"neurônios\", seguida de uma ReLU para introduzir a não linearidade nos dados.\n",
    "- No final temos uma fully connected com 10 \"neurônios\", um para cada classe do nosso dataset. Terminamos com uma softmax para converter os valores em probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet:\n",
    "    \"\"\" Implementation of architecture LeNet-5 \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def build(nRows=32, nCols=32, nChannels=3, nClasses=10, opt='sgd', activation='relu'):\n",
    "        \"\"\" Static method to create LeNet-5 model \"\"\"\n",
    "\n",
    "        # Criando um modelo sequencial\n",
    "        model = keras.Sequential()\n",
    "\n",
    "        # Adicionando uma camada convolucional, relu e max pooling\n",
    "        model.add(Conv2D(20, 5, padding='same', input_shape=(nRows, nCols, nChannels)))\n",
    "        model.add(Activation(activation))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "        # Adicionando mais uma camada convolucional, relu e max pooling\n",
    "        model.add(Conv2D(50, 5, padding='same'))\n",
    "        model.add(Activation(activation))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "        # Adicionando nossa camada fully connected\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(500))\n",
    "        model.add(Activation(activation))\n",
    "\n",
    "        # Adicionando a segunda camada fully connected (final)\n",
    "        model.add(Dense(nClasses))\n",
    "        model.add(Activation('softmax'))\n",
    "\n",
    "        # Compilando o modelo\n",
    "        model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação de uma classe para criar nossa primeira arquitetura NN\n",
    "- Nos baseamos na arquitetura da NN feita para classificar dados do MNIST de digitos.\n",
    "- A nossa primeira arquitetura consiste em 3 hidden layers, com 64, 128 e 64 \"neurônios\" respectivamente (sempre seguidos de uma ReLU para introduzir a não linearidade nos dados).\n",
    "- No final temos uma fully connected com 10 \"neurônios\", um para cada classe do nosso dataset. Terminamos com uma softmax para converter os valores em probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFirstNN:\n",
    "    \"\"\" Implementation of our own NN a architecture \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def build(nRows=32, nCols=32, nChannels=3, nClasses=10, opt='sgd', activation='relu'):\n",
    "        \"\"\" Static method to create our own NN model \"\"\"\n",
    "\n",
    "        # Criando um modelo sequencial\n",
    "        model = Sequential()\n",
    "\n",
    "        # Adicionando uma hidden layer com 64 nodes\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64, input_shape=(nRows, nCols, nChannels)))\n",
    "        model.add(Activation(activation))\n",
    "\n",
    "        # Adicionando uma hidden layer com 128 nodes\n",
    "        model.add(Dense(128))\n",
    "        model.add(Activation(activation))\n",
    "\n",
    "        # Adicionando uma hidden layer com 64 nodes\n",
    "        model.add(Dense(64))\n",
    "        model.add(Activation(activation))\n",
    "\n",
    "        # Adicionando um fully connected (final)\n",
    "        model.add(Dense(nClasses))\n",
    "        model.add(Activation('softmax'))\n",
    "\n",
    "        # Compilando o modelo\n",
    "        model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação de uma classe para criar nossa segunda arquitetura NN\n",
    "- Nos baseamos no site https://towardsdatascience.com/cifar-10-image-classification-in-tensorflow-5b501f7dc77c para construir essa arquitetura.\n",
    "- Ela é mais complexa que a anterior, possuindo 3 hidden layers com 128, 256 e 512 \"neurônios\" respectivamente (sempre seguidos de uma ReLU para introduzir a não linearidade nos dados).\n",
    "- No final temos uma fully connected com 10 \"neurônios\", um para cada classe do nosso dataset. Terminamos com uma softmax para converter os valores em probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySecondNN:\n",
    "    \"\"\" Implementation of our own NN a architecture \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def build(nRows=32, nCols=32, nChannels=3, nClasses=10, opt='sgd', activation='relu'):\n",
    "        \"\"\" Static method to create our own NN model \"\"\"\n",
    "\n",
    "        # Criando um modelo sequencial\n",
    "        model = Sequential()\n",
    "\n",
    "        # Adicionando uma hidden layer com 128 nodes\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, input_shape=(nRows, nCols, nChannels)))\n",
    "        model.add(Activation(activation))\n",
    "\n",
    "        # Adicionando uma segunda hidden layer com 256 nodes\n",
    "        model.add(Dense(256))\n",
    "        model.add(Activation(activation))\n",
    "\n",
    "        # Adicionando uma terceira hidden layer com 512 nodes\n",
    "        model.add(Dense(512))\n",
    "        model.add(Activation(activation))\n",
    "\n",
    "        # Adicionando uma fully connected (final)\n",
    "        model.add(Dense(nClasses))\n",
    "        model.add(Activation('softmax'))\n",
    "\n",
    "        # Compilando o modelo\n",
    "        model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação de uma classe para criar nossa primeira arquitetura CNN\n",
    "- Nos baseamos na arquitetura da LeNet-5 para criar a mesma.\n",
    "- A nossa primeira arquitetura consiste em 4 níves de convolução, com 16, 32, 64 e 128 filtros respectivamente (sempre seguidos de ReLU e MaxPooling2D 2x2).\n",
    "- Em seguida temos uma fully connected com 512 \"neurônios\", seguida de uma ReLU para introduzir a não linearidade nos dados.\n",
    "- No final temos uma fully connected com 10 \"neurônios\", um para cada classe do nosso dataset. Terminamos com uma softmax para converter os valores em probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFirstCNN:\n",
    "    \"\"\" Implementation of our own CNN architecture \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def build(nRows=32, nCols=32, nChannels=3, nClasses=10, opt='sgd', activation='relu'):\n",
    "        \"\"\" Static method to create our own CNN model \"\"\"\n",
    "\n",
    "        # Criando um modelo sequencial\n",
    "        model = keras.Sequential()\n",
    "\n",
    "        # Adicionando uma camada convolucional, relu e max pooling\n",
    "        model.add(Conv2D(16, 5, padding='same', input_shape=(nRows, nCols, nChannels)))\n",
    "        model.add(Activation(activation))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "        # Adicionando uma camada convolucional, relu e max pooling\n",
    "        model.add(Conv2D(32, 5, padding='same'))\n",
    "        model.add(Activation(activation))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "        # Adicionando mais uma camada convolucional, relu e max pooling\n",
    "        model.add(Conv2D(64, 5, padding='same'))\n",
    "        model.add(Activation(activation))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "        # Adicionando mais uma camada convolucional, relu e max pooling\n",
    "        model.add(Conv2D(128, 5, padding='same'))\n",
    "        model.add(Activation(activation))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "        # Adicionando nossa camada fully connected\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512))\n",
    "        model.add(Activation(activation))\n",
    "\n",
    "        # Adicionando a segunda camada fully connected (final)\n",
    "        model.add(Dense(nClasses))\n",
    "        model.add(Activation('softmax'))\n",
    "\n",
    "        # Compilando o modelo\n",
    "        model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação de uma classe para criar nossa segunda arquitetura CNN\n",
    "- Nos baseamos no site https://towardsdatascience.com/cifar-10-image-classification-in-tensorflow-5b501f7dc77c para construir essa arquitetura.\n",
    "- Ela é mais complexa que a anterior, consistindo de 4 níves de convolução, com 16, 32, 64 e 128 filtros respectivamente (sempre seguidos de ReLU e MaxPooling2D 2x2).\n",
    "- Em seguida temos 3 hidden layers com 128, 256 e 512 \"neurônios\" respectivamente (sempre seguidos de uma ReLU para introduzir a não linearidade nos dados).\n",
    "- No final temos uma fully connected com 10 \"neurônios\", um para cada classe do nosso dataset. Terminamos com uma softmax para converter os valores em probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySecondCNN:\n",
    "    \"\"\" Implementation of our own NN a architecture \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def build(nRows=32, nCols=32, nChannels=3, nClasses=10, opt='sgd', activation='relu'):\n",
    "        \"\"\" Static method to create our own NN model \"\"\"\n",
    "\n",
    "        # Criando um modelo sequencial\n",
    "        model = Sequential()\n",
    "\n",
    "        # Adicionando uma camada convolucional, relu e max pooling\n",
    "        model.add(Conv2D(16, 3, padding='same', input_shape=(nRows, nCols, nChannels)))\n",
    "        model.add(Activation(activation))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "        # Adicionando uma camada convolucional, relu e max pooling\n",
    "        model.add(Conv2D(32, 3, padding='same'))\n",
    "        model.add(Activation(activation))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "        # Adicionando mais uma camada convolucional, relu e max pooling\n",
    "        model.add(Conv2D(64, 3, padding='same'))\n",
    "        model.add(Activation(activation))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "        # Adicionando mais uma camada convolucional, relu e max pooling\n",
    "        model.add(Conv2D(128, 3, padding='same'))\n",
    "        model.add(Activation(activation))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "        # Adicionando as camadas fully connected\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, input_shape=(nRows, nCols, nChannels)))\n",
    "        model.add(Activation(activation))\n",
    "\n",
    "        model.add(Dense(256))\n",
    "        model.add(Activation(activation))\n",
    "\n",
    "        model.add(Dense(512))\n",
    "        model.add(Activation(activation))\n",
    "\n",
    "        # Adicionando um fully connected (final)\n",
    "        model.add(Dense(nClasses))\n",
    "        model.add(Activation('softmax'))\n",
    "\n",
    "        # Compilando o modelo\n",
    "        model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escolha dos hiper-parâmetros (learning rate, epoch, batch size)\n",
    "\n",
    "- Para escolhermos os melhores hiper-parâmetros para cada arquitetura usamos uma abordagem de grid search.\n",
    "- Tentamos utilizar a biblioteca scikit-learn para tal, mas não funcionou como esperavamos. Então, optamos por implementar na mão essa mesma ideia.\n",
    "- Resolvemos iterar por exemplo entre os seguintes valores:\n",
    "\n",
    "| learning rate | epochs | batch size |\n",
    "|---------------|--------|------------|\n",
    "|      0.1      |   10   |     64     |\n",
    "|      0.01     |   20   |    128     |\n",
    "|      0.001    |   30   |    256     |\n",
    "\n",
    "- Para uma visualização melhor, treinamos cada uma das redes e usamos o TensorBoard para visualizar o aprendizado com o passar das épocas. O resultado foi o seguinte:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acurácia no treino\n",
    "![Results train accuracy](result_acc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss no treino\n",
    "![Results train loss](result_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acurácia na validação\n",
    "![Results validation accuracy](result_val_acc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss na validação\n",
    "\n",
    "![Results validation loss](result_val_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conseguimos perceber nos plots acima que possuimos um overfit para as redes convolucionais e um underfit para as redes não convolucionais. Testamos treinar com menos épocas e learning rates mais baixos, mas não conseguimos um resultado melhor do que ~68% na acurácia dos testes (este, obtido pelas CNN).\n",
    "\n",
    "- Tentamos treinar até a época onde a loss começa a crescer novamente e a acurácia a diminuir. Porém, no geral, os resultados foram menores do que os vistos usando esses hiper parâmetros a seguir:\n",
    "\n",
    "\n",
    "|             | learning rate | epochs | batch size || accuracy (test) | loss (test) |\n",
    "|-------------|---------------|--------|------------||-----------------|-------------|\n",
    "|  LeNet-5    | 0.01          | 30     | 64         || 0.65            | 1.30        |\n",
    "| MyFirstNN   | 0.01          | 30     | 128        || 0.48            | 1.44        |\n",
    "| MySecondNN  | 0.01          | 30     | 64         || 0.51            | 1.40        |\n",
    "| MyFirstCNN  | 0.01          | 30     | 64         || 0.64            | 1.50        |\n",
    "| MySecondCNN | 0.01          | 30     | 64         || 0.67            | 1.08        |\n",
    "\n",
    "\n",
    "- Mesmo com a melhoria nos resultados, o tempo de treinamento entre redes mais simples foi menor do que em suas versões mais complexas. Sendo assim, devemos realizar um trade-off entre tempo de treino e resultado obtido. Visto que, a melhoria foi de ~3% em média nos testes, talvez fosse melhor optar por uma rede mais simples.\n",
    "\n",
    "- Com base na tabela acima, fomos capazes de ajustar na mão os hiper parâmetros afim de obter resultados melhores. Por exemplo, para a LeNet-5 encontramos que um learning rate de 0.0025, 25 épocas e batch size igual a 32 no possibilitou um resultado com bem menos overfit (ao invés de termos uma acurácia de ~90% nos treinos tivemos de ~70%) e com acurácia e loss similares aos testes passados, as vezes até melhores que 65% e 1.30, principalmente quanto a loss que fica bem próximo de 1, ficando assim ~23% melhor que o valor observado na tabela acima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrizes de confusão\n",
    "\n",
    "- Aqui mostraremos a matriz de confusão para MySecondNN e MySecondCNN, visto que as mesmas foram as que se saíram melhor no teste dentro das suas \"categorias\", ou seja, NN e CNN.\n",
    "- Também mostraremos a matriz de confusão para a LeNet-5 para podermos comparar com as demais redes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo os dados do CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = Cifar10.read_data()\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet-5 (hiper parâmetros melhorados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      " - 12s - loss: 2.0623 - acc: 0.2515 - val_loss: 1.9055 - val_acc: 0.3270\n",
      "Epoch 2/25\n",
      " - 11s - loss: 1.8206 - acc: 0.3588 - val_loss: 1.7420 - val_acc: 0.3816\n",
      "Epoch 3/25\n",
      " - 11s - loss: 1.6646 - acc: 0.4122 - val_loss: 1.6216 - val_acc: 0.4295\n",
      "Epoch 4/25\n",
      " - 10s - loss: 1.5483 - acc: 0.4545 - val_loss: 1.5119 - val_acc: 0.4684\n",
      "Epoch 5/25\n",
      " - 10s - loss: 1.4630 - acc: 0.4840 - val_loss: 1.4044 - val_acc: 0.5011\n",
      "Epoch 6/25\n",
      " - 10s - loss: 1.3954 - acc: 0.5068 - val_loss: 1.3681 - val_acc: 0.5087\n",
      "Epoch 7/25\n",
      " - 10s - loss: 1.3397 - acc: 0.5283 - val_loss: 1.3801 - val_acc: 0.5159\n",
      "Epoch 8/25\n",
      " - 10s - loss: 1.2908 - acc: 0.5468 - val_loss: 1.3114 - val_acc: 0.5375\n",
      "Epoch 9/25\n",
      " - 10s - loss: 1.2465 - acc: 0.5643 - val_loss: 1.2527 - val_acc: 0.5541\n",
      "Epoch 10/25\n",
      " - 10s - loss: 1.2049 - acc: 0.5785 - val_loss: 1.2352 - val_acc: 0.5640\n",
      "Epoch 11/25\n",
      " - 10s - loss: 1.1678 - acc: 0.5914 - val_loss: 1.2718 - val_acc: 0.5486\n",
      "Epoch 12/25\n",
      " - 10s - loss: 1.1326 - acc: 0.6042 - val_loss: 1.1817 - val_acc: 0.5883\n",
      "Epoch 13/25\n",
      " - 10s - loss: 1.0996 - acc: 0.6184 - val_loss: 1.1999 - val_acc: 0.5851\n",
      "Epoch 14/25\n",
      " - 10s - loss: 1.0696 - acc: 0.6272 - val_loss: 1.1937 - val_acc: 0.5719\n",
      "Epoch 15/25\n",
      " - 10s - loss: 1.0419 - acc: 0.6383 - val_loss: 1.1390 - val_acc: 0.6021\n",
      "Epoch 16/25\n",
      " - 10s - loss: 1.0115 - acc: 0.6501 - val_loss: 1.1243 - val_acc: 0.6092\n",
      "Epoch 17/25\n",
      " - 10s - loss: 0.9852 - acc: 0.6604 - val_loss: 1.0994 - val_acc: 0.6160\n",
      "Epoch 18/25\n",
      " - 10s - loss: 0.9600 - acc: 0.6676 - val_loss: 1.0787 - val_acc: 0.6221\n",
      "Epoch 19/25\n",
      " - 10s - loss: 0.9334 - acc: 0.6788 - val_loss: 1.0911 - val_acc: 0.6167\n",
      "Epoch 20/25\n",
      " - 10s - loss: 0.9093 - acc: 0.6860 - val_loss: 1.0682 - val_acc: 0.6299\n",
      "Epoch 21/25\n",
      " - 10s - loss: 0.8860 - acc: 0.6937 - val_loss: 1.0259 - val_acc: 0.6467\n",
      "Epoch 22/25\n",
      " - 10s - loss: 0.8635 - acc: 0.7037 - val_loss: 1.0461 - val_acc: 0.6407\n",
      "Epoch 23/25\n",
      " - 10s - loss: 0.8414 - acc: 0.7104 - val_loss: 1.0232 - val_acc: 0.6390\n",
      "Epoch 24/25\n",
      " - 10s - loss: 0.8165 - acc: 0.7193 - val_loss: 1.0993 - val_acc: 0.6246\n",
      "Epoch 25/25\n",
      " - 10s - loss: 0.7956 - acc: 0.7257 - val_loss: 1.0450 - val_acc: 0.6447\n",
      "\n",
      "test_acc: 0.6447\n",
      "test_loss: 1.0450025494575501\n"
     ]
    }
   ],
   "source": [
    "# Construindo o modelo e treinando o mesmo\n",
    "model = LeNet.build(opt=SGD(lr=0.0025))\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=25,\n",
    "          batch_size=32,\n",
    "          validation_data=(test_images, test_labels), \n",
    "          verbose=2)\n",
    "\n",
    "# Exibindo os resultados no teste\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=0)\n",
    "print('\\ntest_acc:', test_acc)\n",
    "print('test_loss:', test_loss)\n",
    "\n",
    "# Classificando as imagens e criando a matriz de confusão\n",
    "_, (_, true_labels) = cifar10.load_data()\n",
    "preds = model.predict_classes(test_images)\n",
    "confusion_matrix = tf.Session().run(tf.math.confusion_matrix(labels=true_labels, predictions=preds))\n",
    "\n",
    "# Normalizando a matriz de confusão\n",
    "confusion_matrix = np.around(confusion_matrix.astype('float') / confusion_matrix.sum(axis=1)[:, np.newaxis], decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.75, 0.  , 0.02, 0.02, 0.01, 0.02, 0.01, 0.02, 0.1 , 0.06],\n",
       "       [0.03, 0.63, 0.  , 0.02, 0.01, 0.01, 0.01, 0.01, 0.06, 0.24],\n",
       "       [0.1 , 0.  , 0.39, 0.08, 0.09, 0.18, 0.05, 0.05, 0.02, 0.03],\n",
       "       [0.03, 0.01, 0.03, 0.43, 0.04, 0.32, 0.04, 0.04, 0.02, 0.05],\n",
       "       [0.04, 0.  , 0.05, 0.07, 0.5 , 0.12, 0.04, 0.14, 0.03, 0.01],\n",
       "       [0.02, 0.  , 0.01, 0.11, 0.02, 0.72, 0.02, 0.06, 0.01, 0.01],\n",
       "       [0.01, 0.01, 0.04, 0.1 , 0.04, 0.09, 0.66, 0.02, 0.01, 0.03],\n",
       "       [0.02, 0.  , 0.01, 0.04, 0.03, 0.12, 0.  , 0.74, 0.01, 0.04],\n",
       "       [0.07, 0.02, 0.  , 0.02, 0.  , 0.02, 0.  , 0.01, 0.82, 0.04],\n",
       "       [0.04, 0.04, 0.  , 0.02, 0.  , 0.02, 0.01, 0.02, 0.05, 0.8 ]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
